{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52062d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS notebook is broken and needs refactoring to take only the last part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac5756e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import PIL\n",
    "import tensorflow as tf\n",
    "import pathlib\n",
    "import os\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f94138",
   "metadata": {},
   "outputs": [],
   "source": [
    " tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899bbba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# silence warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28bb8078",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model config\n",
    "\n",
    "batch_size = 32\n",
    "img_width = 320\n",
    "img_height = 320 #img_width * 2\n",
    "rgb_channels = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc37a4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = pathlib.Path('samples/train')\n",
    "\n",
    "image_count = len(list(data_dir.glob('*/*.jpg'))) + len(list(data_dir.glob('*/*.png'))) + len(list(data_dir.glob('*/*.jpeg')))\n",
    "image_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5377f6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "  data_dir,\n",
    "  validation_split=0.2,\n",
    "  subset=\"training\",\n",
    "  seed=123,\n",
    "  image_size=(img_height, img_width),\n",
    "  batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26d93f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Introspect the object\n",
    "print(train_ds.__dict__.keys(), \"\\n\")\n",
    "print(train_ds.file_paths[0:5], \"\\n\")\n",
    "print(train_ds.class_names, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae45b66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "  data_dir,\n",
    "  validation_split=0.2,\n",
    "  subset=\"validation\",\n",
    "  seed=123,\n",
    "  image_size=(img_height, img_width),\n",
    "  batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25827ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = train_ds.class_names\n",
    "print(class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02e8c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for image_batch, labels_batch in train_ds:\n",
    "    print(image_batch.shape)\n",
    "    print(labels_batch.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e6fdb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure dataset for better performance\n",
    "\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "train_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)\n",
    "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73df1fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize colors from 0-255 to 0-1\n",
    "\n",
    "normalization_layer = layers.Rescaling(1./255)\n",
    "\n",
    "normalized_ds = train_ds.map(lambda x, y: (normalization_layer(x), y))\n",
    "image_batch, labels_batch = next(iter(normalized_ds))\n",
    "\n",
    "# Notice the pixel values are now in `[0,1]`.\n",
    "first_image = image_batch[0]\n",
    "print(np.min(first_image), np.max(first_image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c34e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the model\n",
    "\n",
    "num_classes = len(class_names)\n",
    "input_shape = (img_height, img_width, rgb_channels)\n",
    "\n",
    "# Add GaussianNoise layer for robustness\n",
    "noise = layers.GaussianNoise(0.01, input_shape=input_shape)\n",
    "\n",
    "model = Sequential([\n",
    "    noise,\n",
    "  layers.Rescaling(1./255, input_shape=input_shape),\n",
    "  layers.Conv2D(16, 3, padding='same', activation='relu'),\n",
    "  layers.MaxPooling2D(),\n",
    "  layers.Conv2D(32, 3, padding='same', activation='relu'),\n",
    "  layers.MaxPooling2D(),\n",
    "  layers.Conv2D(64, 3, padding='same', activation='relu'),\n",
    "  layers.MaxPooling2D(),\n",
    "  layers.Flatten(),\n",
    "  layers.Dense(128, activation='relu'),\n",
    "  layers.Dense(num_classes) # output layer,\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aceb472",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973e875c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6702d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.build(input_shape=input_shape)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7924306",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "\n",
    "epochs=10\n",
    "history = model.fit(\n",
    "  train_ds,\n",
    "  validation_data=val_ds,\n",
    "  epochs=epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "721e5af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc9616a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing results\n",
    "\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs_range = range(epochs)\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, loss, label='Training Loss')\n",
    "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c287be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on new data\n",
    "\n",
    "pred_data_dir = pathlib.Path('samples/test')\n",
    "pred_images = list(pred_data_dir.glob('**/*.jpg')) + list(pred_data_dir.glob('**/*.png'))\n",
    "\n",
    "print(pred_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036c603e",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 80\n",
    "\n",
    "for im in pred_images:\n",
    "    img_path_str = os.path.join(os.getcwd(), str(im))\n",
    "    img_title = os.path.basename(img_path_str)\n",
    "    #img_path = tf.keras.utils.get_file(os.path.basename(img_title), origin=img_path_str)\n",
    "\n",
    "    img = tf.keras.utils.load_img(\n",
    "        img_path_str, target_size=(img_height, img_width)\n",
    "    )\n",
    "    img_array = tf.keras.utils.img_to_array(img)\n",
    "    img_array = tf.expand_dims(img_array, 0) # Create a batch\n",
    "\n",
    "    predictions = model.predict(img_array)\n",
    "    score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "    print(\n",
    "        img_title,\n",
    "        \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "        .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff68d1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize what each layer is doing\n",
    "# https://www.analyticsvidhya.com/blog/2020/11/tutorial-how-to-visualize-feature-maps-directly-from-cnn-layers/\n",
    "\n",
    "layer_names = [layer.name for layer in model.layers]\n",
    "layer_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53001a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import img_to_array, load_img\n",
    "\n",
    "layer_outputs = [layer.output for layer in model.layers]\n",
    "visual_model = tf.keras.models.Model(inputs=model.input, outputs=layer_outputs)\n",
    "\n",
    "image_path= r\"samples/test/scr04.png\"\n",
    "img = load_img(image_path, target_size=(img_height, img_width))\n",
    "img_array = img_to_array(img)    \n",
    "print(img_array.shape)\n",
    "#input_img = x.reshape((1,) + x.shape)                   \n",
    "#input_img /= 255.0\n",
    "\n",
    "plt.imshow(img_array.astype(\"uint8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b81fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape, to be able to pass it as prediction\n",
    "input_img = img_array.reshape((1,) + img_array.shape)                   \n",
    "input_img /= 255.0\n",
    "print(input_img.shape)\n",
    "\n",
    "# rescale\n",
    "input_img = input_img / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c9b8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict\n",
    "\n",
    "feature_maps = visual_model.predict(input_img)\n",
    "print(len(feature_maps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30afa4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "score2 = tf.nn.softmax(model.predict(input_img)[0])\n",
    "np.argmax(score2)\n",
    "print(\n",
    "    \"'{}' belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(image_path, class_names[np.argmax(score2)], 100 * np.max(score2))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc6d497",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9717dd31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting the graph\n",
    "for layer_name, feature_map in zip(layer_names, feature_maps):\n",
    "    if len(feature_map.shape) == 4: # 4-dimensions layer\n",
    "        channels = feature_map.shape[-1]\n",
    "        size_h = feature_map.shape[1]\n",
    "        size_w = feature_map.shape[2]\n",
    "        #print(layer_name)\n",
    "        #print('feature_map.shape:', feature_map.shape)\n",
    "        \n",
    "        size = feature_map.shape[1]\n",
    "        display_grid = np.zeros((size_h, size_w * channels))\n",
    "        #print('display_grid.shape:', display_grid.shape)\n",
    "        for i in range(channels):\n",
    "            x = feature_map[0, :, :, i]\n",
    "            x -= x.mean()\n",
    "            x /= x.std()\n",
    "            x *= img_width\n",
    "            x += img_width\n",
    "            x = np.clip(x, 0, 255).astype('uint8')\n",
    "            # print(x.shape)\n",
    "            # tile each filter into this big horizontal grid\n",
    "            display_grid[:, i*size_w: (i + 1) * size_w] = x\n",
    "        \n",
    "        #print(\"\\n\")\n",
    "        scale = 20. / channels\n",
    "        plt.figure(figsize=(scale * channels, scale))\n",
    "        plt.title(layer_name)\n",
    "        plt.grid(False)\n",
    "        plt.imshow(display_grid, aspect='auto', cmap='viridis')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250f96bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f738f74",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML Python 3.8",
   "language": "python",
   "name": "mlp38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
